Chapter 1: What is Reinforcement Learning
AI
    Machine Learning
        Tasks 
            Supervised Learning
            Unsupervised Learning
            Reinforcement Learning
                Concepts: 
                    experience: (current_state, action, next_state, reward)
                    return:The sum of rewards collected in a single episode
                    policy:a universal plan, i.e., what to do for every state
                    state-value function: also known as value function or V-function or V_pi(s)
                    action-value function: also known as Q-function or Q_pi(s,a)
                    policy evaluation(P77): 
                    policy improvement(P84):
                    policy iteration = policy evaluation + policy improvement
                    value iteration(P93):iterate the optimal value
                Categories:
                    policy-based:approximate policies
                    value-based:approximate value functions
                    actor-critic:approximate both policies and value functions
                    model-based:approximate models, that is, learn mappings from observations to new observations and/or rewards
                    model-free:don't approximate models 
        Methods and Toolboxes
            Traditional Machine Learning Methods
            Deep Learning


Frozen Lake:
    state: one single variable called location l, the variable l can take values from {0,1,...15}
    state space: The set of all possible states, which can be infinie. but for frozen lake, it's {0,1,...15}
        initial state:
        terminal state:absorbing state, all transitions from a terminal state ends in itself, with probability 1 and 0 reward.
    action: one single variable called direction d, the variable takes value from {up,down,left,right}
    action space: set of all possible actions, which can be infinie, but for frozen lake, it's {up,down,left,right}
    transition function:T(s,a,s') returns a probability, or T(s,a) returns a dict: T(s,a)={s1:p1,s2:p2,...,sn:pn}, p1+p2+...+pn=1
        probability distribution: p(s'|s,a)
    reward function:r(s,a,s'), margin with s', we get r(s,a), then margin with a, we get r(s)
    horizon:also called: epoch, cycle, iteration, or even interaction
        finite horizon:agent knows the task will terminate in a finite number of time steps
        greedy horizon:the episode terminates immediately after one interaction
        infinite horizon:the agent doesn’t have a predetermined time step limit. indefinite horizon task
    episode, trial, period, or stage:sequence of consecutive time steps from the beginning to the end of an episodic task


MDP(S,A,T,R,S_theta,Gamma,H)
    S:state space
    A:action space
    T:transition function
    R:reward signal
    S_theta:inital distribution
    Gamma:discount factor
    H:Horizon


POMDP(S,A,T,R,S_theta,Gamma,H,O,Epsilon)
    S:state space
    A:action space
    T:transition function
    R:reward signal
    S_theta:inital distribution
    Gamma:discount factor
    H:Horizon
    O:observation space
    Epsilon:an emission probability Epsilon that defines the probability of showing an observation Ot given a state St




*************** Chapter 4 ***************
Exploration and Exploitation:
    basic exploration:
        pure exploitation: always greedy, easily get stucked
        pure exploration: always random, gather information
        epsilon-greedy: almost always greedy but sometimes explore
        decaying epsilon-greedy: first totally random, then decay epsilon, with less random
            linearly decaying epsilon-greedy strategy
            exponentially decaying epsilon-greedy strategy
        optimistic initialization:start with optimistic Q values and counts(>1), decreasing rewards, and always greedy
    advanced exploration:
        softmax:do softmax to Q(s,a), get a distribution of Q values
        upper confidence bound(UCB): encourage exploring actions that is less selected
        thompson sampling:regard Q values as random variables and sample values from their distributions, thus introducing randomness



*************** Chapter 5 ***************
Estimating the value of policies:
    MC methods:(not biased, but high variance)
        First-visit Monte Carlo: Improving estimates after each episode
        Every-visit Monte Carlo: Improving estimates every time we see a state
    Temporal-difference learning(TD): Improving estimates after each step(biased, but low variance)
    Methods between MC and TD:
        N-step TD learning: Improving estimates after a couple of steps(More like MC, not biased, but high variance)
        Forward-view TD(λ): Improving estimates of all visited states
        Backward-view TD(λ) or TD(λ) for short: Improving estimates of all visited states after each step(More like TD, biased, but low variance)



*************** Chapter 6 ***************
generalized policy iteration (GPI): estimate, improve, estimate, improve...
Seperate things:
    Gather Data: experiences
    Learn from Data: Learn Value function or models
    Improve from Data:
RL problem types:
    Non-interactive learning problems
    Interactive learning problems

Monte Carlo control: Improving policies after each episode. Control means involving two phrases: 
    policy-evaluation: Monte Carlo prediction for Q(s,a), not V(s)
    policy-improvement: a decaying epsilon-greedy action-selection strategy

SARSA: Improving policies after each step
    policy-evaluation: TD prediction for Q(s,a), on-policy
    policy-improvement: a decaying epsilon-greedy action-selection strategy

Q-learning: 
    policy-evaluation: TD prediction for Q(s,a), off-policy
    policy-improvement: a decaying epsilon-greedy action-selection strategy

double Q-learning:
    policy-evaluation: TD prediction for Q(s,a), off-policy, but using two Q estimations
    policy-improvement: a decaying epsilon-greedy action-selection strategy



*************** Chapter 7 ***************
SARSA(λ): Improving policies after each step based on multi-step estimates
    policy-evaluation(for learning):
        TD prediction for Q(s,a)
        on-policy
        accumulating trace or replacing trace 
    policy-improvement(for exploration): a decaying epsilon-greedy action-selection strategy 

Watkins’s Q(λ): Decoupling behavior from learning, again
    policy-evaluation: 
        TD prediction for Q(s,a)
        off-policy: clear eligible traces if next action is not optimal ?
        accumulating trace or replacing trace 
    policy-improvement: a decaying epsilon-greedy action-selection strategy

Model-based RL:
    Dyna-Q
        sample uniformly
    Trajectory sampling
        sample from the current state



*************** Chapter 8 ***************
RL learning style:
    Sequential vs One-step
        sequential: multi-steps
        one-shot: just one step
    Evaluative vs Supervised
        evaluative: no supervision, must explore
        Supervised: correct answer is provided
    Sampled vs Exhaustive:
        sampled: impossible to sample all possible feedbacks because the problem is more complext
        exhaustive: can explore and sample all the possible feedback

Neural Fitted Q-learning (NFQ):
    approximate Q(s,a)
    4 in -> 2 out
    generalized policy iteration
    td target
    epsilon-greedy exploration: constant epsilon


*************** Chapter 9 ***************
DQN(Nature DQN or Vanilla DQN):Deep Q-networks
    two networks:
        online network
        target network
    replay buffer
    exploration strategies:
        linearly decaying epsilon-greedy exploration strategy: linearly decaying epsilon
        exponentially decaying epsilon-greedy exploration strategy: exponentially decaying epsilon
        softmax: select action based on softmax of Q(s,a), a distribution

Double DQN(DDQN):
    two networks:
        online network: a_star = argmax Q(s,a), tells which action has the max value, that is a_star
        target network: a_val = Q(s, a_start), tell the value of a_star, as a cross-validation
    gradient clipping:
        don't update network's weight too much, even loss is large, because we are not always sure we are updating in the right direction in RL


*************** Chapter 10 ***************
Dueling DDQN:only architecture changes, no algorithm changes
    Core Idea: taking the worst action in a good state could be better than taking the best action in a bad state
    Q(s,a) = V(s) + A(s,a) - Sigma_a'(A(s,a')) / N (N is number of actions)
    Polyak Averaging:updating target network gradually

Prioritized Experience Replay:
    calculate p_i:  
        absolute TD error: pi = |TD error| + e
        by rank(sort experiences by absolute TD error in a decending way): pi = 1/rank(i)
    calculate P using pi:
        P(i) = p_i**alpha/Sigma_k(p_k**alpha)
    weighted importance sampling(compensate the biases introduced by prioritized experience repaly): the bigger P(i) is, the smaller W_i(the weight) is, the smaller P(i) is, the bigger W_i is


*************** Chapter 11 ***************
REINFORCE:using full Monte Carlo returns, so variance is hight
    generate a trajactory tau: S_0,A_0,R_1,S_1,...,S_T-1,A_T-1,R_T,S_T(totally T steps)
    calculate the return of each action: G_t(tau):for each t in (0, T-1)
    calculate logpa of each action using NN, log(pi(A_t|S_t)) for each t in (0,T-1)
    loss = -(discounts * [G_0(tau),...,G_T-1(tau)] * [log(pi(A_0|S_0)),...,log(pi(A_T-1|S_T-1))]).mean()

Vanilla Policy Gradient(REINFORCE with baseline):still using full Monte Carlo returns, but use A(s,a) to reduce variance
    add a new NN to estimate the V(s)
    roll out a full episode, calculate estimation of EQ(s,a) and EV(s)
    calculate A(s,a) = EQ(s,a) - V(s), use - (A(s,a)*logpa + entropy) as loss to update Q(s,a) network
    use EV(s) - V(S)  as loss to update V(s) network

A3C:
    using N-step return: rollout n steps, using immediate rewards and predicted V(s)
    using N-worker asynchronously:
        load global weights to local weights
        interact with local env using local weights to calculate local gradient
        update global gradient using local gradient
        optimized global weight using global optimizer
        reload gloable weights to local weights
        no locks, no blocking, totally different from the hands-on book

Generalized advantage estimation:
    lambda-target

A2C:
    n working environment
    lambda-target


*************** Chapter 12 ***************
DDPG:deep deterministic policy gradient
    use a network to reutrn optimal actions directly
    use a network to return action values directly
    add Gaussian noise to actions

Twin-Delayed DDPG:TD3
    use twin network
    add noise to target actions(the action used to calculate the targets) for robust policy network, not only to behavior actions(for exploration)
    delayed update of policy network

SAC:
    update twin networks independently
    add entropy to target values
    don't use target action smooth as in TD3
    update alpha: 
        for actions with great probability, 
            increase alphas, so reduce Q targets and loss, so small updates for Q;  
            increase alphas, so increase Policy loss, so large updates for Policy, make Policy network generate Actions with greater Q values
        for actions with low probability, 
            decrease alphas, so increase Q targets and loss, so large updates.
            decreate alphas, so decrease Policy loss, so small updates for Policy
        so, actually, encourage agent to try new actions?

PPO:
    clear enough