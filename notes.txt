policy-based methods:
Cross-Entropy
REINFORCEMENT
Policy-Gradient


# Policy Gradient: improvements on REINFORCEMENT
# 1. full episode: no full episodes are needed, using N-steps rewards as an estimation of Q(s,a). This might not as accurate as full episode, but is more efficient
# 2. high gradient varianceï¼šmoving average of rewards, to avoid high gradient variance
# 3. exploration: avoid local optimal policy using entropy bonus
# 4. correlation between sample: parallel environments are used to avoid sample correlations
# 5. clip gradients: Gradients are clipped to improve training stability

######################### On-Policy and Off-Policy #########################
On-Policy: 
1. our current policy interact with the environments
2. the interaction generated some fresh experiences
3. we must use these fresh experiences to train the network
4. sample inefficiency: throw away experiences after one single training

Off-Policy:
1. replay buffer

######################### Value-based and Policy-based #########################
Value-based Off-policy
Policy-based On-policy