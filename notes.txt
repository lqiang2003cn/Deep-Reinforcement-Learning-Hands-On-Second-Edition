policy-based methods:
Cross-Entropy
REINFORCEMENT
Policy-Gradient


# Policy Gradient: improvements on REINFORCEMENT
# 1. full episode: no full episodes are needed, using N-steps rewards as an estimation of Q(s,a). This might not as accurate as full episode, but is more efficient
# 2. high gradient varianceï¼šmoving average of rewards, to avoid high gradient variance
# 3. exploration: avoid local optimal policy using entropy bonus
# 4. correlation between sample: parallel environments are used to avoid sample correlations
# 5. clip gradients: Gradients are clipped to improve training stability
